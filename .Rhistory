# Grid of lambdas
lambda_values <- exp(seq(log(1e-4), log(1000), length.out = 40))
rmse <- function(pred, obs) sqrt(mean((pred - obs)^2))
val_scores <- data.frame(lambda = lambda_values, RMSE = NA_real_)
models <- vector("list", length(lambda_values))
# Fit on TRAIN for each lambda; evaluate on VALIDATION
for (i in seq_along(lambda_values)) {
print(i)
models[[i]] <- lab4::ridgereg(form, data = train_dat, lambda = lambda_values[i])
pred_i <- predict(models[[i]], newdata = valid_dat)
val_scores$RMSE[i] <- rmse(pred_i, valid_dat$arr_delay)
}
knitr::opts_chunk$set(
collapse = TRUE, comment = "#>", fig.width = 7, fig.height = 4.5
)
set.seed(732094)   # reproducibility
library(nycflights13)
library(dplyr)
library(tidyr)
library(lubridate)
library(caret)
library(ggplot2)
library(lab4)  # exposes ridgereg(),
use_subset <- FALSE  # set TRUE if you need faster vignette builds
fl <- nycflights13::flights %>%
mutate(
hour = sched_dep_time %/% 100L,                  # hour-of-day (0..23)
wday = wday(time_hour, label = TRUE, abbr = TRUE) # factor weekday
) %>%
select(
# target
arr_delay,
# join keys for weather
year, month, day, hour, origin,
# plausible predictors
carrier, dest, distance, air_time, dep_time, sched_dep_time, dep_delay,
wday
)
wx <- nycflights13::weather %>%
select(year, month, day, hour, origin,
temp, dewp, humid, wind_dir, wind_speed, wind_gust,
precip, pressure, visib)
dat <- fl %>%
inner_join(wx, by = c("year","month","day","hour","origin")) %>%
# remove rows with missing target or essential predictors
filter(!is.na(arr_delay)) %>%
# optional thinning for vignette speed
{ if (use_subset) sample_n(., 50000) else . } %>%
# drop clearly non-predictive IDs / leakage-esque columns after join
select(-year, -day) %>%
mutate(
# interactions (Step 2)
temp_humid         = temp * humid,
wind_precip        = wind_speed * precip,
visib_precip       = visib * precip,
# simple transforms to stabilize
log_distance       = log1p(distance),
dep_hour           = factor(hour),    # schedule effect
month              = factor(month),
origin             = factor(origin),
dest               = factor(dest),
carrier            = factor(carrier)
) %>%
select(
arr_delay,
month, wday, dep_hour, origin, dest, carrier,
distance, log_distance, air_time, dep_time, dep_delay,
temp, dewp, humid, wind_dir, wind_speed, wind_gust, precip, pressure, visib,
temp_humid, wind_precip, visib_precip
)
# Final cleanup of remaining NAs (weather sometimes missing)
dat <- dat %>% tidyr::drop_na(arr_delay)
dim(dat)
set.seed(20251021)
N <- nrow(dat)
n_test  <- floor(0.05 * N)
n_train <- floor(0.80 * N)
# remainder goes to validation to ensure 5/80/15 (sums to N)
n_valid <- N - n_test - n_train
# stratify on sign of arr_delay to keep a mix (simple stratification)
strata <- factor(ifelse(dat$arr_delay > 0, "late", "not_late"))
idx_all <- caret::createDataPartition(strata, p = 1, list = FALSE)
dat <- dat[idx_all, , drop = FALSE]  # (kept order; idx_all is 1:n here)
all_idx <- seq_len(N)
test_idx  <- sample(all_idx, n_test)
remain    <- setdiff(all_idx, test_idx)
train_idx <- sample(remain, n_train)
valid_idx <- setdiff(remain, train_idx)
stopifnot(length(test_idx) + length(train_idx) + length(valid_idx) == N)
train_dat <- dat[train_idx, , drop = FALSE]
valid_dat <- dat[valid_idx, , drop = FALSE]
test_dat  <- dat[test_idx,  , drop = FALSE]
sapply(list(train=nrow(train_dat), valid=nrow(valid_dat), test=nrow(test_dat)), identity)
# ---- Option B: Leakage-safe imputation (RECOMMENDED) ------------------------
# Impute NUMERIC predictors with the TRAINING medians, then apply to valid/test.
# For FACTORS, replace NA with "Unknown" and align levels to training.
# 1) Identify predictor columns by type (exclude the target)
pred_cols_train <- setdiff(names(train_dat), "arr_delay")
num_vars <- names(Filter(is.numeric, train_dat[pred_cols_train]))
fac_vars <- names(Filter(is.factor,  train_dat[pred_cols_train]))
# 2) Compute training medians for numeric predictors
num_medians <- vapply(train_dat[num_vars], function(x) median(x, na.rm = TRUE), numeric(1))
# 3) Helper to impute numerics using a named vector of medians
impute_numeric <- function(df, medians) {
for (v in names(medians)) {
if (!v %in% names(df)) next
idx <- is.na(df[[v]])
if (any(idx)) df[[v]][idx] <- medians[[v]]
}
df
}
# Apply numeric imputation
train_dat <- impute_numeric(train_dat, num_medians)
valid_dat <- impute_numeric(valid_dat, num_medians)
test_dat  <- impute_numeric(test_dat,  num_medians)
# 4) Handle factor NAs and level alignment
#    - On TRAIN, explicitly mark NA as "Unknown"
#    - On VALID/TEST, coerce to the TRAIN levels (+ "Unknown" if needed)
fill_factor_unknown <- function(x, levels_ref) {
# Turn into character, fill NA with "Unknown", then coerce using reference levels
xc <- as.character(x)
xc[is.na(xc)] <- "Unknown"
# ensure "Unknown" is in the level set
levels_final <- union(levels_ref, "Unknown")
factor(xc, levels = levels_final)
}
for (v in fac_vars) {
# training: add explicit "Unknown" if needed
train_dat[[v]] <- forcats::fct_explicit_na(train_dat[[v]], na_level = "Unknown")
ref_levels <- levels(train_dat[[v]])
# validation/test: fill NAs as "Unknown" and align to training levels
valid_dat[[v]] <- fill_factor_unknown(valid_dat[[v]], ref_levels)
test_dat[[v]]  <- fill_factor_unknown(test_dat[[v]],  ref_levels)
}
# 5) Final sanity check: there should be no NA in predictors now
stopifnot(
all(colSums(is.na(train_dat[pred_cols_train])) == 0),
all(colSums(is.na(valid_dat[pred_cols_train])) == 0),
all(colSums(is.na(test_dat [pred_cols_train])) == 0)
)
ridgereg_caret <- list(
type = "Regression",
library = c("lab4"),
parameters = data.frame(
parameter = "lambda", class = "numeric", label = "Ridge Penalty"
),
grid = function(x, y, len = NULL, search = "grid") {
if (!is.null(len) && len > 0) {
data.frame(lambda = exp(seq(log(1e-4), log(100), length.out = len)))
} else {
data.frame(lambda = exp(seq(log(1e-4), log(1000), length.out = 40)))
}
},
fit = function(x, y, wts, param, lev, last, classProbs, ...) {
dat <- as.data.frame(x); dat$.y <- y
lab4::ridgereg(.y ~ ., data = dat, lambda = param$lambda)
},
predict = function(modelFit, newdata, submodels = NULL) {
predict(modelFit, newdata = newdata)
},
prob = NULL
)
# Modeling formula: predict arrival delay from engineered features
form <- arr_delay ~ .
# caret will one-hot encode factors through model.matrix inside ridgereg()
ctrl_none <- trainControl(method = "none")
lambda_grid <- data.frame(lambda = exp(seq(log(1e-4), log(1000), length.out = 40)))
set.seed(1337)
# lambda hyperparam search grid
lambda_values <- exp(seq(log(1e-4), log(1000), length.out = 10))
print(paste0("Searching with lambdas: ", lambda_values))
rmse <- function(pred, obs) sqrt(mean((pred - obs)^2))
val_scores <- data.frame(lambda = lambda_values, RMSE = NA_real_)
models <- vector("list", length(lambda_values))
# Fit on TRAIN for each lambda; evaluate on VALIDATION
for (i in seq_along(lambda_values)) {
print(i)
models[[i]] <- lab4::ridgereg(form, data = train_dat, lambda = lambda_values[i])
pred_i <- predict(models[[i]], newdata = valid_dat)
val_scores$RMSE[i] <- rmse(pred_i, valid_dat$arr_delay)
}
best_idx    <- which.min(val_scores$RMSE)
best_lambda <- val_scores$lambda[best_idx]
# Final model at best lambda (re-fit on TRAIN for cleanliness)
final_ridge <- lab4::ridgereg(form, data = train_dat, lambda = best_lambda)
# Manual validation loop over the candidate lambdas
rmse <- function(pred, obs) sqrt(mean((pred - obs)^2))  # RMSE per lab hint
val_scores <- ridge_grid_fits$results[, "lambda", drop = FALSE]
knitr::opts_chunk$set(
collapse = TRUE, comment = "#>", fig.width = 7, fig.height = 4.5
)
set.seed(732094)   # reproducibility
library(nycflights13)
library(dplyr)
library(tidyr)
library(lubridate)
library(caret)
library(ggplot2)
library(forcats)
library(lab4)  # exposes ridgereg()
use_subset <- FALSE  # set TRUE if you need faster vignette builds
fl <- nycflights13::flights %>%
mutate(
hour = sched_dep_time %/% 100L,                  # hour-of-day (0..23)
wday = wday(time_hour, label = TRUE, abbr = TRUE) # factor weekday
) %>%
select(
# target
arr_delay,
# join keys for weather
year, month, day, hour, origin,
# plausible predictors
carrier, dest, distance, air_time, dep_time, sched_dep_time, dep_delay,
wday
)
wx <- nycflights13::weather %>%
select(year, month, day, hour, origin,
temp, dewp, humid, wind_dir, wind_speed, wind_gust,
precip, pressure, visib)
dat <- fl %>%
inner_join(wx, by = c("year","month","day","hour","origin")) %>%
# remove rows with missing target or essential predictors
filter(!is.na(arr_delay)) %>%
# optional thinning for vignette speed
{ if (use_subset) sample_n(., 50000) else . } %>%
# drop clearly non-predictive IDs / leakage-esque columns after join
select(-year, -day) %>%
mutate(
# interactions (Step 2)
temp_humid         = temp * humid,
wind_precip        = wind_speed * precip,
visib_precip       = visib * precip,
# simple transforms to stabilize
log_distance       = log1p(distance),
dep_hour           = factor(hour),    # schedule effect
month              = factor(month),
origin             = factor(origin),
dest               = factor(dest),
carrier            = factor(carrier)
) %>%
select(
arr_delay,
month, wday, dep_hour, origin, dest, carrier,
distance, log_distance, air_time, dep_time, dep_delay,
temp, dewp, humid, wind_dir, wind_speed, wind_gust, precip, pressure, visib,
temp_humid, wind_precip, visib_precip
)
# Final cleanup of remaining NAs in target (predictors will be handled later)
dat <- dat %>% tidyr::drop_na(arr_delay)
dim(dat)
set.seed(20251021)
N <- nrow(dat)
n_test  <- floor(0.05 * N)
n_train <- floor(0.80 * N)
# remainder goes to validation to ensure 5/80/15 (sums to N)
n_valid <- N - n_test - n_train
all_idx  <- seq_len(N)
test_idx <- sample(all_idx, n_test)
remain   <- setdiff(all_idx, test_idx)
train_idx <- sample(remain, n_train)
valid_idx <- setdiff(remain, train_idx)
stopifnot(length(test_idx) + length(train_idx) + length(valid_idx) == N)
train_dat <- dat[train_idx, , drop = FALSE]
valid_dat <- dat[valid_idx, , drop = FALSE]
test_dat  <- dat[test_idx,  , drop = FALSE]
sapply(list(train=nrow(train_dat), valid=nrow(valid_dat), test=nrow(test_dat)), identity)
# ---- Leakage-safe imputation (Option B) -------------------------------------
# Impute NUMERIC predictors using TRAINING medians, then apply to valid/test.
# For FACTORS, replace NA with "Unknown" and align levels to training.
# 1) Identify predictor columns by type (exclude the target)
pred_cols_train <- setdiff(names(train_dat), "arr_delay")
num_vars <- names(Filter(is.numeric, train_dat[pred_cols_train]))
fac_vars <- names(Filter(is.factor,  train_dat[pred_cols_train]))
# 2) Compute training medians for numeric predictors
num_medians <- vapply(train_dat[num_vars], function(x) median(x, na.rm = TRUE), numeric(1))
# 3) Helper to impute numerics using a named vector of medians
impute_numeric <- function(df, medians) {
for (v in names(medians)) {
if (!v %in% names(df)) next
idx <- is.na(df[[v]])
if (any(idx)) df[[v]][idx] <- medians[[v]]
}
df
}
# Apply numeric imputation
train_dat <- impute_numeric(train_dat, num_medians)
valid_dat <- impute_numeric(valid_dat, num_medians)
test_dat  <- impute_numeric(test_dat,  num_medians)
# 4) Handle factor NAs and level alignment
#    - On TRAIN, explicitly mark NA as "Unknown"
#    - On VALID/TEST, coerce to the TRAIN levels (+ "Unknown" if needed)
fill_factor_unknown <- function(x, levels_ref) {
xc <- as.character(x)
xc[is.na(xc)] <- "Unknown"
levels_final <- union(levels_ref, "Unknown")
factor(xc, levels = levels_final)
}
for (v in fac_vars) {
# training: add explicit "Unknown" if needed
train_dat[[v]] <- forcats::fct_explicit_na(train_dat[[v]], na_level = "Unknown")
ref_levels <- levels(train_dat[[v]])
# validation/test: fill NAs as "Unknown" and align to training levels
valid_dat[[v]] <- fill_factor_unknown(valid_dat[[v]], ref_levels)
test_dat[[v]]  <- fill_factor_unknown(test_dat[[v]],  ref_levels)
}
# 5) Final sanity check: there should be no NA in predictors now
stopifnot(
all(colSums(is.na(train_dat[pred_cols_train])) == 0),
all(colSums(is.na(valid_dat[pred_cols_train])) == 0),
all(colSums(is.na(test_dat [pred_cols_train])) == 0)
)
# Modeling formula: predict arrival delay from engineered features
form <- arr_delay ~ .
# RMSE definition (per lab hint)
rmse <- function(pred, obs) sqrt(mean((pred - obs)^2))
# Log-spaced lambda search grid
lambda_values <- exp(seq(log(1e-4), log(1000), length.out = 25))
message("Searching with lambdas: ", paste(signif(lambda_values, 4), collapse = ", "))
# Fit on TRAIN for each lambda; evaluate on VALIDATION
val_scores <- data.frame(lambda = lambda_values, RMSE = NA_real_)
models <- vector("list", length(lambda_values))
for (i in seq_along(lambda_values)) {
models[[i]] <- lab4::ridgereg(form, data = train_dat, lambda = lambda_values[i])
pred_i <- predict(models[[i]], newdata = valid_dat)
val_scores$RMSE[i] <- rmse(pred_i, valid_dat$arr_delay)
}
# Pick the best lambda by validation RMSE
best_idx    <- which.min(val_scores$RMSE)
best_lambda <- val_scores$lambda[best_idx]
# Final model at best lambda (re-fit on TRAIN)
final_ridge <- lab4::ridgereg(form, data = train_dat, lambda = best_lambda)
# Show top candidates
dplyr::arrange(val_scores, RMSE) |> head(10)
ggplot(val_scores, aes(x = lambda, y = RMSE)) +
geom_line() + geom_point() +
scale_x_log10() +
geom_vline(xintercept = best_lambda, linetype = "dashed") +
labs(title = "Validation RMSE across lambda (ridge)",
subtitle = paste("Best lambda =", signif(best_lambda, 4)),
x = "lambda (log scale)", y = "Validation RMSE")
final_ridge <- lab4::ridgereg(form, data = train_dat, lambda = best_lambda)
pred_test <- predict(final_ridge, newdata = test_dat)
test_rmse <- sqrt(mean((pred_test - test_dat$arr_delay)^2))
data.frame(
best_lambda = best_lambda,
test_RMSE   = test_rmse
)
sessionInfo()
devtools::build()
devtools::load_all()
devtools::build_vignettes()
knitr::opts_chunk$set(
collapse = TRUE, comment = "#>", fig.width = 7, fig.height = 4.5
)
set.seed(1337) # reproducibility
library(nycflights13)
library(dplyr)
library(tidyr)
library(lubridate)
library(caret)
library(ggplot2)
library(forcats)
library(lab4)  # exposes ridgereg()
use_subset <- FALSE  # set TRUE if you need faster vignette builds
fl <- nycflights13::flights %>%
mutate(
hour = sched_dep_time %/% 100L,                  # hour-of-day (0..23)
wday = wday(time_hour, label = TRUE, abbr = TRUE) # factor weekday
) %>%
select(
# target
arr_delay,
# join keys for weather
year, month, day, hour, origin,
# plausible predictors
carrier, dest, distance, air_time, dep_time, sched_dep_time, dep_delay,
wday
)
wx <- nycflights13::weather %>%
select(year, month, day, hour, origin,
temp, dewp, humid, wind_dir, wind_speed, wind_gust,
precip, pressure, visib)
dat <- fl %>%
inner_join(wx, by = c("year","month","day","hour","origin")) %>%
# remove rows with missing target or essential predictors
filter(!is.na(arr_delay)) %>%
# optional thinning for vignette speed
{ if (use_subset) sample_n(., 50000) else . } %>%
# drop clearly non-predictive IDs / leakage-esque columns after join
select(-year, -day) %>%
mutate(
# interactions (Step 2)
temp_humid         = temp * humid,
wind_precip        = wind_speed * precip,
visib_precip       = visib * precip,
# simple transforms to stabilize
log_distance       = log1p(distance),
dep_hour           = factor(hour),    # schedule effect
month              = factor(month),
origin             = factor(origin),
dest               = factor(dest),
carrier            = factor(carrier)
) %>%
select(
arr_delay,
month, wday, dep_hour, origin, dest, carrier,
distance, log_distance, air_time, dep_time, dep_delay,
temp, dewp, humid, wind_dir, wind_speed, wind_gust, precip, pressure, visib,
temp_humid, wind_precip, visib_precip
)
# Final cleanup of remaining NAs in target (predictors will be handled later)
dat <- dat %>% tidyr::drop_na(arr_delay)
dim(dat)
set.seed(1337)
N <- nrow(dat)
n_test  <- floor(0.05 * N)
n_train <- floor(0.80 * N)
# remainder goes to validation to ensure 5/80/15 (sums to N)
n_valid <- N - n_test - n_train
all_idx  <- seq_len(N)
test_idx <- sample(all_idx, n_test)
remain   <- setdiff(all_idx, test_idx)
train_idx <- sample(remain, n_train)
valid_idx <- setdiff(remain, train_idx)
stopifnot(length(test_idx) + length(train_idx) + length(valid_idx) == N)
train_dat <- dat[train_idx, , drop = FALSE]
valid_dat <- dat[valid_idx, , drop = FALSE]
test_dat  <- dat[test_idx,  , drop = FALSE]
sapply(list(train=nrow(train_dat), valid=nrow(valid_dat), test=nrow(test_dat)), identity)
# 1) Identify predictor columns by type (exclude the target)
pred_cols_train <- setdiff(names(train_dat), "arr_delay")
num_vars <- names(Filter(is.numeric, train_dat[pred_cols_train]))
fac_vars <- names(Filter(is.factor,  train_dat[pred_cols_train]))
# 2) Compute training medians for numeric predictors
num_medians <- vapply(train_dat[num_vars], function(x) median(x, na.rm = TRUE), numeric(1))
# 3) Helper to impute numerics using a named vector of medians
impute_numeric <- function(df, medians) {
for (v in names(medians)) {
if (!v %in% names(df)) next
idx <- is.na(df[[v]])
if (any(idx)) df[[v]][idx] <- medians[[v]]
}
df
}
# Apply numeric imputation
train_dat <- impute_numeric(train_dat, num_medians)
valid_dat <- impute_numeric(valid_dat, num_medians)
test_dat  <- impute_numeric(test_dat,  num_medians)
# 4) Handle factor NAs and level alignment
#    - On TRAIN, explicitly mark NA as "Unknown"
#    - On VALID/TEST, coerce to the TRAIN levels (+ "Unknown" if needed)
fill_factor_unknown <- function(x, levels_ref) {
xc <- as.character(x)
xc[is.na(xc)] <- "Unknown"
levels_final <- union(levels_ref, "Unknown")
factor(xc, levels = levels_final)
}
for (v in fac_vars) {
# training: add explicit "Unknown" if needed
train_dat[[v]] <- forcats::fct_explicit_na(train_dat[[v]], na_level = "Unknown")
ref_levels <- levels(train_dat[[v]])
# validation/test: fill NAs as "Unknown" and align to training levels
valid_dat[[v]] <- fill_factor_unknown(valid_dat[[v]], ref_levels)
test_dat[[v]]  <- fill_factor_unknown(test_dat[[v]],  ref_levels)
}
# 5) Final sanity check: there should be no NA in predictors now
stopifnot(
all(colSums(is.na(train_dat[pred_cols_train])) == 0),
all(colSums(is.na(valid_dat[pred_cols_train])) == 0),
all(colSums(is.na(test_dat [pred_cols_train])) == 0)
)
# Modeling formula: predict arrival delay from engineered features
form <- arr_delay ~ .
# RMSE definition (per lab hint)
rmse <- function(pred, obs) sqrt(mean((pred - obs)^2))
# Log-spaced lambda search grid
lambda_values <- exp(seq(log(1e-2), log(10), length.out = 10))
message("Searching with lambdas: ", paste(signif(lambda_values, 4), collapse = ", "))
# Fit on TRAIN for each lambda; evaluate on VALIDATION
val_scores <- data.frame(lambda = lambda_values, RMSE = NA_real_)
models <- vector("list", length(lambda_values))
for (i in seq_along(lambda_values)) {
models[[i]] <- lab4::ridgereg(form, data = train_dat, lambda = lambda_values[i])
pred_i <- predict(models[[i]], newdata = valid_dat)
val_scores$RMSE[i] <- rmse(pred_i, valid_dat$arr_delay)
}
# Pick the best lambda by validation RMSE
best_idx    <- which.min(val_scores$RMSE)
best_lambda <- val_scores$lambda[best_idx]
# Final model at best lambda (re-fit on TRAIN)
final_ridge <- lab4::ridgereg(form, data = train_dat, lambda = best_lambda)
# Show top candidates
dplyr::arrange(val_scores, RMSE) |> head(10)
ggplot(val_scores, aes(x = lambda, y = RMSE)) +
geom_line() + geom_point() +
scale_x_log10() +
geom_vline(xintercept = best_lambda, linetype = "dashed") +
labs(title = "Validation RMSE across lambda (ridge)",
subtitle = paste("Best lambda =", signif(best_lambda, 4)),
x = "lambda (log scale)", y = "Validation RMSE")
final_ridge <- lab4::ridgereg(form, data = train_dat, lambda = best_lambda)
pred_test <- predict(final_ridge, newdata = test_dat)
test_rmse <- sqrt(mean((pred_test - test_dat$arr_delay)^2))
data.frame(
best_lambda = best_lambda,
test_RMSE   = test_rmse
)
sessionInfo()
devtools::test()
devtools::check()
devtools::check()
devtools::check()
