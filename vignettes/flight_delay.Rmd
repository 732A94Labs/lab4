---
title: "Flight delay prediction with ridgereg"
author: 
  - "Nils Fahrni"
  - "Felix Unterleiter"
date: "`r format(Sys.Date())`"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Flight delay prediction with ridgereg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>", fig.width = 7, fig.height = 4.5
)
set.seed(1337) # reproducibility
```

# 1. Goal and grading requirements

This vignette implements Bonus Lab **Problem 1.2.1 (*)** exactly:

- Use **nycflights13::weather** and **nycflights13::flights**; remove variables with little/no predictive value (Step 1).
- Add **extra weather data** and **create interaction effects** believed useful (Step 2).
- Split into **test (5%)**, **train (80%)**, **validation (15%)** using **caret** (Step 3).
- Train **ridge** models for a grid of \\(\\lambda\\), evaluate **validation RMSE**, and select the best \\(\\lambda\\) (Step 4). The RMSE definition follows the lab hint.
- Finally, **predict the test set** and report **test RMSE** (Step 5).

We rely on your package’s `ridgereg()` and `predict.ridgereg()` methods.

```{r packages}
library(nycflights13)
library(dplyr)
library(tidyr)
library(lubridate)
library(caret)
library(ggplot2)
library(forcats)
library(lab4)  # exposes ridgereg()
```

# 2. Data preparation (Steps 1–2)

We use **arrival delay** as the target (`arr_delay`, minutes). We **join hourly weather** to each flight (by `year`, `month`, `day`, `hour`, `origin`) and engineer a few **domain-motivated interactions**:

- `temp:humid` (hot + humid often slows ops),  
- `wind_speed:precip` (wind + rain),  
- `visib:precip` (visibility under precipitation).

We also remove ID-like or leakage-prone variables (e.g., `year`, `time_hour`, `tailnum`, `flight`) and keep features that are plausibly predictive (schedule, day-of-week, carrier, origin/dest, distance). This satisfies “remove eventual variables you do not believe to have a predictive value” and “add extra weather data + interactions.”

> **Performance note.** The full join is moderately large. If vignette build time is a concern, set `use_subset <- TRUE` to sample flights down while preserving methodology (allowed per the problem text).

```{r build-modeling-frame, message=FALSE}
use_subset <- FALSE  # set TRUE if you need faster vignette builds

fl <- nycflights13::flights %>%
  mutate(
    hour = sched_dep_time %/% 100L,                  # hour-of-day (0..23)
    wday = wday(time_hour, label = TRUE, abbr = TRUE) # factor weekday
  ) %>%
  select(
    # target
    arr_delay,
    # join keys for weather
    year, month, day, hour, origin,
    # plausible predictors
    carrier, dest, distance, air_time, dep_time, sched_dep_time, dep_delay,
    wday
  )

wx <- nycflights13::weather %>%
  select(year, month, day, hour, origin,
         temp, dewp, humid, wind_dir, wind_speed, wind_gust,
         precip, pressure, visib)

dat <- fl %>%
  inner_join(wx, by = c("year","month","day","hour","origin")) %>%
  # remove rows with missing target or essential predictors
  filter(!is.na(arr_delay)) %>%
  # optional thinning for vignette speed
  { if (use_subset) sample_n(., 50000) else . } %>%
  # drop clearly non-predictive IDs / leakage-esque columns after join
  select(-year, -day) %>%
  mutate(
    # interactions (Step 2)
    temp_humid         = temp * humid,
    wind_precip        = wind_speed * precip,
    visib_precip       = visib * precip,
    # simple transforms to stabilize
    log_distance       = log1p(distance),
    dep_hour           = factor(hour),    # schedule effect
    month              = factor(month),
    origin             = factor(origin),
    dest               = factor(dest),
    carrier            = factor(carrier)
  ) %>%
  select(
    arr_delay,
    month, wday, dep_hour, origin, dest, carrier,
    distance, log_distance, air_time, dep_time, dep_delay,
    temp, dewp, humid, wind_dir, wind_speed, wind_gust, precip, pressure, visib,
    temp_humid, wind_precip, visib_precip
  )

# Final cleanup of remaining NAs in target (predictors will be handled later)
dat <- dat %>% tidyr::drop_na(arr_delay)

dim(dat)
```

# 3. Split into **test (5%)**, **train (80%)**, **validation (15%)** (Step 3)

We select indices to match the exact proportions required by the lab.

```{r split-sets}
set.seed(1337)

N <- nrow(dat)
n_test  <- floor(0.05 * N)
n_train <- floor(0.80 * N)
# remainder goes to validation to ensure 5/80/15 (sums to N)
n_valid <- N - n_test - n_train

all_idx  <- seq_len(N)
test_idx <- sample(all_idx, n_test)
remain   <- setdiff(all_idx, test_idx)
train_idx <- sample(remain, n_train)
valid_idx <- setdiff(remain, train_idx)

stopifnot(length(test_idx) + length(train_idx) + length(valid_idx) == N)

train_dat <- dat[train_idx, , drop = FALSE]
valid_dat <- dat[valid_idx, , drop = FALSE]
test_dat  <- dat[test_idx,  , drop = FALSE]

sapply(list(train=nrow(train_dat), valid=nrow(valid_dat), test=nrow(test_dat)), identity)
```

```{r impute-after-split, message=FALSE}
# 1) Identify predictor columns by type (exclude the target)
pred_cols_train <- setdiff(names(train_dat), "arr_delay")
num_vars <- names(Filter(is.numeric, train_dat[pred_cols_train]))
fac_vars <- names(Filter(is.factor,  train_dat[pred_cols_train]))

# 2) Compute training medians for numeric predictors
num_medians <- vapply(train_dat[num_vars], function(x) median(x, na.rm = TRUE), numeric(1))

# 3) Helper to impute numerics using a named vector of medians
impute_numeric <- function(df, medians) {
  for (v in names(medians)) {
    if (!v %in% names(df)) next
    idx <- is.na(df[[v]])
    if (any(idx)) df[[v]][idx] <- medians[[v]]
  }
  df
}

# Apply numeric imputation
train_dat <- impute_numeric(train_dat, num_medians)
valid_dat <- impute_numeric(valid_dat, num_medians)
test_dat  <- impute_numeric(test_dat,  num_medians)

# 4) Handle factor NAs and level alignment
#    - On TRAIN, explicitly mark NA as "Unknown"
#    - On VALID/TEST, coerce to the TRAIN levels (+ "Unknown" if needed)
fill_factor_unknown <- function(x, levels_ref) {
  xc <- as.character(x)
  xc[is.na(xc)] <- "Unknown"
  levels_final <- union(levels_ref, "Unknown")
  factor(xc, levels = levels_final)
}

for (v in fac_vars) {
  # training: add explicit "Unknown" if needed
  train_dat[[v]] <- forcats::fct_explicit_na(train_dat[[v]], na_level = "Unknown")
  ref_levels <- levels(train_dat[[v]])
  # validation/test: fill NAs as "Unknown" and align to training levels
  valid_dat[[v]] <- fill_factor_unknown(valid_dat[[v]], ref_levels)
  test_dat[[v]]  <- fill_factor_unknown(test_dat[[v]],  ref_levels)
}

# 5) Final sanity check: there should be no NA in predictors now
stopifnot(
  all(colSums(is.na(train_dat[pred_cols_train])) == 0),
  all(colSums(is.na(valid_dat[pred_cols_train])) == 0),
  all(colSums(is.na(test_dat [pred_cols_train])) == 0)
)
```

# 4. Ridge models across \\(\\lambda\\); choose by **validation RMSE** (Steps 4–5)

We scan a **log-spaced grid** of \\(\\lambda\\). For each candidate, we fit on **training** and compute **validation RMSE**, then select the best \\(\\lambda\\) by minimum RMSE.

```{r model-formula}
# Modeling formula: predict arrival delay from engineered features
form <- arr_delay ~ .
```

```{r train-grid-and-validate, message=FALSE}
# RMSE definition (per lab hint)
rmse <- function(pred, obs) sqrt(mean((pred - obs)^2))

# Log-spaced lambda search grid
lambda_values <- exp(seq(log(1e-2), log(10), length.out = 10))
message("Searching with lambdas: ", paste(signif(lambda_values, 4), collapse = ", "))

# Fit on TRAIN for each lambda; evaluate on VALIDATION
val_scores <- data.frame(lambda = lambda_values, RMSE = NA_real_)
models <- vector("list", length(lambda_values))

for (i in seq_along(lambda_values)) {
  models[[i]] <- lab4::ridgereg(form, data = train_dat, lambda = lambda_values[i])
  pred_i <- predict(models[[i]], newdata = valid_dat)
  val_scores$RMSE[i] <- rmse(pred_i, valid_dat$arr_delay)
}

# Pick the best lambda by validation RMSE
best_idx    <- which.min(val_scores$RMSE)
best_lambda <- val_scores$lambda[best_idx]

# Final model at best lambda (re-fit on TRAIN)
final_ridge <- lab4::ridgereg(form, data = train_dat, lambda = best_lambda)

# Show top candidates
dplyr::arrange(val_scores, RMSE) |> head(10)
```

Visualizing validation performance across \\(\\lambda\\):

```{r plot-lambda}
ggplot(val_scores, aes(x = lambda, y = RMSE)) +
  geom_line() + geom_point() +
  scale_x_log10() +
  geom_vline(xintercept = best_lambda, linetype = "dashed") +
  labs(title = "Validation RMSE across lambda (ridge)",
       subtitle = paste("Best lambda =", signif(best_lambda, 4)),
       x = "lambda (log scale)", y = "Validation RMSE")
```

# 5. Finalize model at best \\(\\lambda\\) and **report test RMSE** (Step 5)

We refit on the **training** set with the chosen \\(\\lambda\\), then evaluate **test RMSE**.

```{r final-test}
final_ridge <- lab4::ridgereg(form, data = train_dat, lambda = best_lambda)

pred_test <- predict(final_ridge, newdata = test_dat)
test_rmse <- sqrt(mean((pred_test - test_dat$arr_delay)^2))

data.frame(
  best_lambda = best_lambda,
  test_RMSE   = test_rmse
)
```

# 6. Session info
```{r session-info}
sessionInfo()
```
